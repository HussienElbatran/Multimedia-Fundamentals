{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multimedia Processing Course - Part 5: Text Processing\n",
                "\n",
                "Text is a fundamental type of multimedia data. In this notebook, we explore how to manipulate, analyze, and extract information from text using Python.\n",
                "\n",
                "**Content:**\n",
                "1.  **Level 1 (Basic)**: String Manipulation and Tokenization.\n",
                "2.  **Level 2 (Intermediate)**: Text Cleaning, Frequency Analysis, and Word Clouds.\n",
                "3.  **Level 3 (Advanced)**: Sentiment Analysis and Text Similarity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required libraries (run once if needed)\n",
                "# !pip install nltk wordcloud matplotlib\n",
                "\n",
                "import re\n",
                "import string\n",
                "from collections import Counter\n",
                "\n",
                "import nltk\n",
                "import matplotlib.pyplot as plt\n",
                "from wordcloud import WordCloud\n",
                "\n",
                "# Download required NLTK data\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "nltk.download('vader_lexicon', quiet=True)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "\n",
                "from nltk.tokenize import word_tokenize, sent_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.sentiment import SentimentIntensityAnalyzer\n",
                "\n",
                "print(\"Libraries loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explanation\n",
                "We import the core libraries: `re` for regular expressions, `nltk` for natural language processing, `Counter` for frequency counting, `WordCloud` for visualization, and `matplotlib` for plotting."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Level 1: String Manipulation and Tokenization\n",
                "The first step in text processing is breaking raw text into meaningful units called **tokens** (words or sentences)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample text for the entire notebook\n",
                "sample_text = \"\"\"\n",
                "Multimedia is a combination of text, images, audio, and video. It is widely used in education,\n",
                "entertainment, and communication. Text processing is one of the most important areas in\n",
                "multimedia systems. Natural language processing (NLP) enables computers to understand,\n",
                "interpret, and generate human language. Text data is everywhere: emails, websites, social\n",
                "media, books, and more. Analyzing text helps us extract valuable information and insights.\n",
                "\"\"\"\n",
                "\n",
                "# Basic string operations\n",
                "print(\"Original length (chars):\", len(sample_text))\n",
                "print(\"Uppercase:\", sample_text[:50].upper())\n",
                "print(\"Lowercase:\", sample_text[:50].lower())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explanation\n",
                "Python strings have built-in methods like `.upper()`, `.lower()`, and `len()` for basic manipulation. These are the first step when working with raw text data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sentence tokenization\n",
                "sentences = sent_tokenize(sample_text.strip())\n",
                "print(f\"Number of sentences: {len(sentences)}\")\n",
                "for i, sent in enumerate(sentences):\n",
                "    print(f\"  Sentence {i+1}: {sent}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explanation\n",
                "`sent_tokenize` from NLTK splits the text into a list of sentences. It handles abbreviations and punctuation intelligently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Word tokenization\n",
                "words = word_tokenize(sample_text.lower())\n",
                "print(f\"Total tokens (words + punctuation): {len(words)}\")\n",
                "print(\"First 20 tokens:\", words[:20])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explanation\n",
                "`word_tokenize` splits text into individual words and punctuation marks. We convert to lowercase first to normalize the text."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Level 2: Text Cleaning, Frequency Analysis, and Word Cloud\n",
                "Raw text is messy. We need to remove stop words, punctuation, and then analyze what words appear most often."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Text cleaning: remove punctuation and stop words\n",
                "stop_words = set(stopwords.words('english'))\n",
                "\n",
                "# Keep only alphabetic tokens and remove stop words\n",
                "clean_words = [\n",
                "    word for word in words\n",
                "    if word.isalpha() and word not in stop_words\n",
                "]\n",
                "\n",
                "print(f\"Tokens after cleaning: {len(clean_words)}\")\n",
                "print(\"Clean words sample:\", clean_words[:20])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explanation\n",
                "**Stop words** are common words (like 'the', 'is', 'in') that carry little meaning. Removing them reduces noise. `.isalpha()` filters out numbers and punctuation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Word frequency analysis\n",
                "word_freq = Counter(clean_words)\n",
                "most_common = word_freq.most_common(10)\n",
                "\n",
                "print(\"Top 10 most frequent words:\")\n",
                "for word, count in most_common:\n",
                "    print(f\"  '{word}': {count} time(s)\")\n",
                "\n",
                "# Bar chart of frequencies\n",
                "words_list, counts_list = zip(*most_common)\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.bar(words_list, counts_list, color='steelblue')\n",
                "plt.title('Top 10 Word Frequencies')\n",
                "plt.xlabel('Word')\n",
                "plt.ylabel('Count')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explanation\n",
                "`Counter` from Python's built-in `collections` module counts occurrences of each element. `.most_common(10)` returns the 10 most frequent words as (word, count) pairs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Word Cloud visualization\n",
                "wordcloud_text = ' '.join(clean_words)\n",
                "\n",
                "wc = WordCloud(\n",
                "    width=800,\n",
                "    height=400,\n",
                "    background_color='white',\n",
                "    colormap='viridis',\n",
                "    max_words=50\n",
                ").generate(wordcloud_text)\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.imshow(wc, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.title('Word Cloud of Sample Text', fontsize=16)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explanation\n",
                "A **Word Cloud** is a visual representation where more frequent words appear larger. `WordCloud` from the `wordcloud` library generates it. We join our cleaned words into a single string and pass it to `.generate()`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Level 3: Sentiment Analysis and Text Similarity\n",
                "**Sentiment analysis** determines whether a piece of text is positive, negative, or neutral. **Text similarity** measures how alike two pieces of text are."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sentiment Analysis using NLTK's VADER\n",
                "sia = SentimentIntensityAnalyzer()\n",
                "\n",
                "test_sentences = [\n",
                "    \"Text processing is absolutely amazing and very powerful!\",\n",
                "    \"This task is terrible and extremely difficult.\",\n",
                "    \"Multimedia combines text, audio, image, and video.\"\n",
                "]\n",
                "\n",
                "print(\"Sentiment Analysis Results:\")\n",
                "print(\"-\" * 50)\n",
                "for sentence in test_sentences:\n",
                "    scores = sia.polarity_scores(sentence)\n",
                "    label = 'POSITIVE' if scores['compound'] >= 0.05 else ('NEGATIVE' if scores['compound'] <= -0.05 else 'NEUTRAL')\n",
                "    print(f\"Text   : {sentence}\")\n",
                "    print(f\"Scores : {scores}\")\n",
                "    print(f\"Label  : {label}\")\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explanation\n",
                "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon-based sentiment analyzer. `polarity_scores()` returns:\n",
                "- `neg`, `neu`, `pos`: proportion of negative, neutral, and positive tokens.\n",
                "- `compound`: a single normalized score between -1 (most negative) and +1 (most positive)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Text Similarity using Jaccard Similarity\n",
                "def jaccard_similarity(text1, text2):\n",
                "    \"\"\"Compute Jaccard similarity between two texts.\"\"\"\n",
                "    set1 = set(word_tokenize(text1.lower()))\n",
                "    set2 = set(word_tokenize(text2.lower()))\n",
                "    intersection = set1.intersection(set2)\n",
                "    union = set1.union(set2)\n",
                "    return len(intersection) / len(union) if union else 0.0\n",
                "\n",
                "text_a = \"Multimedia includes text, images, audio, and video.\"\n",
                "text_b = \"Text and audio are key parts of multimedia systems.\"\n",
                "text_c = \"Python is a popular programming language for data science.\"\n",
                "\n",
                "sim_ab = jaccard_similarity(text_a, text_b)\n",
                "sim_ac = jaccard_similarity(text_a, text_c)\n",
                "\n",
                "print(f\"Similarity (A vs B): {sim_ab:.2f}  <- Both are about multimedia\")\n",
                "print(f\"Similarity (A vs C): {sim_ac:.2f}  <- Different topics\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explanation\n",
                "**Jaccard Similarity** measures overlap between two sets of words:\n",
                "\n",
                "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
                "\n",
                "A score of 1.0 means the texts are identical (same words). A score of 0.0 means no words in common. It is simple but effective for short text comparison."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat": 4,
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
